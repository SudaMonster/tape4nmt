global {

  # 0. basic settings
  # 0.0 language
  src=so
  tgt=en

  # 0.1 data path
  data=(section:
    train=(side:
        src="/home/pkoehn/experiment/material-so-en/training/corpus.1.so"
        tgt="/home/pkoehn/experiment/material-so-en/training/corpus.1.en"
      )
    dev=(side:
      src="/home/pkoehn/experiment/material-so-en/tuning/input.tc.1"
      tgt="/home/pkoehn/experiment/material-so-en/tuning/reference.tc.1"
    )
    test=(side:
      src="/home/pkoehn/experiment/material-so-en/evaluation/analysis.input.tc.1"
      tgt="/home/pkoehn/experiment/material-so-en/evaluation/analysis.reference.tok.1"
    )
  )

  # 0.2 submitter info
  submitter="sge"
  cpu_resource_flags="-l 'mem_free=8g,ram_free=8g'"
  gpu_resource_flags="-l 'hostname=b1[123456789]|c*,gpu=1,mem_free=8g,ram_free=8g'"
  action_flags="-m ae -M xutai_ma@jhu.edu"

  # 0.4 environmental variables
  python_env="pytorch-v0.4.1-p36-cu80"

  # 1. preprocess
  # 1.1 bpe
  bpe_operations=(NumberOfBPE: 2000 5000 10000 20000 40000)
  bpe_input=(RunTasksBeforeBPE:
    yes=$out@truecase
    no=$data
  )
  # 1.2 clean
  clean_minlen=1
  clean_maxlen=100

  #2. Training

  #2.1 training paths
  save_checkpoint_steps=2000
  keep_checkpoint=-1
  gpuid=0
  seed=1234
  param_init=0.1
  train_from=""
  pre_word_vecs_enc=""
  pre_word_vecs_dec=""

  #2.2 training optimization
  batch_size=64
  batch_type='sents'
  normalization='sents'
  valid_steps=2000
  valid_batch_size=64
  max_generator_batches=32
  train_steps=40000
  optim='adam'
  max_grad_norm=5
  dropout=0.3
  adam_beta1=0.9
  adam_beta2=0.999
  label_smoothing=0.0
  learning_rate=0.001
  learning_rate_decay=0.8
  start_decay_steps=10000
  decay_steps=2000
  #2.3 log
  report_every=100
  #2.4 seq2seq model
  src_word_vec_size=512
  tgt_word_vec_size=512
  feat_merge='concat'
  feat_vec_size=-1
  feat_vec_exponent=0.7
  encoder_type='rnn'
  decoder_type='rnn'
  enc_layers=2
  dec_layers=2
  rnn_size=512
  rnn_type='LSTM'
  global_attention='general'
  global_attention_function='softmax'
  self_attn_type="scaled-dot"
  generator_function="log_softmax"
}


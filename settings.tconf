global {

  # GLOBAL/PREPROCESSING CONFIGURATIONS
  working_dir="/export/b18/shuoyangd/projects/fairseq-test/exps"

  src=so
  tgt=en
  tgt_lang=en
  Ratio=1
  MaxLen=80


  njobs=50
  submitter="sge"
  resource_flags="-l 'mem_free=2g,ram_free=2g'"
  action_flags="-m ae -M xutai_ma@jhu.edu"
  #pyenv="/home/shuoyangd/pyenv/py3t4/bin/activate"
  conda_env="mxnet_v1.2.1"

  train_data=(side:
      src="/home/pkoehn/experiment/material-so-en/training/corpus.1.so"
      tgt="/home/pkoehn/experiment/material-so-en/training/corpus.1.en"
      )

  dev_data=(side:
      src="/home/pkoehn/experiment/material-so-en/tuning/input.tc.1"
      tgt="/home/pkoehn/experiment/material-so-en/tuning/reference.tc.1"
    )
  test_data=(side:
      src="/home/pkoehn/experiment/material-so-en/evaluation/analysis.input.tc.1"
      tgt="/home/pkoehn/experiment/material-so-en/evaluation/analysis.reference.tok.1"
    )

  src_truecaser=(UseExistingTruecaser:
    yes=""
    no=$out@train_truecaser
  )
  tgt_truecaser=(UseExistingTruecaser:
    yes=""
    no=$out@train_truecaser
  )

  tokenized_tgt=(DoTokenize:
    yes=$out@tgt_tokenize
    no=$out@aggregate[side:tgt]
  )

  bpe_input=(DoTokenizeAndTruecase:
    yes=$out@truecase
    no=$out@aggregate
  )

  sgm_dev=""
  sgm_test=""

  multi_bleu="true"
  nist_bleu="true"
  
  bpe_operations=(NumberOfBPE: 2000 5000 10000 20000 40000)

  # TRAINING CONFIGURATIONS
  # all default is consistent with nematus
  train_train_from="" # if there is a previous model to start with
  train_train_from_state_dict="" # if there is a previous dict to start with
  train_start_epoch="" # if trained for certain amount of epochs previously

  train_batch_size="80"
  train_optim="adam"
  train_dropout="0.1"
  train_lr="0.0005"
  train_lr_min="1e-09"
  train_lr_shrink="0.5"
  train_lr_scheduler="inverse_sqrt"
  train_warmup_init_lr="1e-07"
  train_warmup_updates="0"
  # train_warmup_updates="4000"
  train_criterion="label_smoothed_cross_entropy"
  train_label_smoothing="0.1"
  train_clip_norm=(ClipNorm: 0.0 0.1 0.5 1 5)
  train_max_tokens="4000"
  train_arch=(Architecture: conv="fconv" transformer="transformer" fconv_iwslt_de_en="fconv_iwslt_de_en" transformer_iwslt_de_en="transformer_iwslt_de_en")
  train_share_all_embeddings="True"
  train_adam_beta1="0.9"
  train_adam_beta2="0.98"

  # TEST CONFIGURATIONS
  test_model_selection_strategy="acc"
  test_max_sent_length="300"
  test_beam_size="12"
  test_batch_size="32"
  test_replace_unk="True"
  test_remove_bpe=""
}


global {
  
  # GLOBAL/PREPROCESSING CONFIGURATIONS
  working_dir="/export/b18/shuoyangd/projects/tape4nmt"
  pyenv="" # if you have virtualenv environment, add them here

  SRC=zh
  TRG=en
  trg_lang=English
  Ratio=1
  MaxLen=80

  bpe_operations=49500

  njobs=50
  email="" # set your own email address so we can email you about the jobs

  resource_flags="-l 'mem_free=2g,ram_free=2g'"
  bigmem_resource_flags="-l 'mem_free=16g,ram_free=16g'"
  gpu_resource_flags="-l 'hostname=c*,gpu=1,mem_free=12g,ram_free=12g'"
  bigmem_gpu_resource_flags="-l 'hostname=c*,gpu=1,mem_free=12g,ram_free=12g'"

  action_flags="-m ae -M $email"
  gpu_action_flags="-m ae -M $email -q g.q"

  train_data=(
    side:
      src="../data/corpus.sents.zh.10000"
      trg="../data/corpus.sents.en.10000"
  )

  dev_data=(
    side:
      src="../data/eval05.zh"
      trg="../data/eval05.en"
  )

  devtest_data=(
    side:
      src="../data/eval08.zh"
      trg="../data/eval08.en"
  )

  wrap_template="../data/eval08.zh"

  sgm_dev=""
  sgm_devtest="true"

  multi_bleu="true"
  nist_bleu="true"

  # TRAINING CONFIGURATIONS
  # all default is consistent with nematus
  train_train_from="" # if there is a previous model to start with
  train_train_from_state_dict="" # if there is a previous dict to start with
  train_start_epoch="" # if trained for certain amount of epochs previously
  
  train_layers="2"
  train_rnn_size="1024"
  train_word_vec_size="300"
  # train_brnn="true" # deprecated!
  train_batch_size="80"
  train_epochs="20" 
  train_optim="adadelta"
  train_dropout="0.2" # implementation may not be consistent with nematus
  train_learning_rate="1.0"
  train_pre_word_vecs_enc=""
  train_pre_word_vecs_dec=""
  train_pre_word_vecs_enc_features_prefix=""
  train_enc_fix=""
  train_encoder_type="brnn"

  # TEST CONFIGURATIONS
  test_model_selection_strategy="acc"
  test_max_sent_length="150"
  test_beam_size="12"
  test_batch_size="1"
  test_replace_unk="true"
}
